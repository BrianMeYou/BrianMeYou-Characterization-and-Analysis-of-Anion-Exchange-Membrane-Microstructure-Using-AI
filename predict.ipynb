{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T06:20:22.324476Z","iopub.status.busy":"2024-03-26T06:20:22.324165Z","iopub.status.idle":"2024-03-26T06:20:26.559923Z","shell.execute_reply":"2024-03-26T06:20:26.559104Z","shell.execute_reply.started":"2024-03-26T06:20:22.324447Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from efficientnet_pytorch import EfficientNet\n","from efficientnet_pytorch.utils import url_map, url_map_advprop, get_model_params\n","import functools\n","import torch.utils.model_zoo as model_zoo\n","from typing import Optional, Union, List\n","import cv2\n","import matplotlib.pyplot as plt\n","import albumentations as albu\n","from tqdm import tqdm as tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T06:20:26.569294Z","iopub.status.busy":"2024-03-26T06:20:26.569028Z","iopub.status.idle":"2024-03-26T06:20:26.602075Z","shell.execute_reply":"2024-03-26T06:20:26.601246Z","shell.execute_reply.started":"2024-03-26T06:20:26.569264Z"},"trusted":true},"outputs":[],"source":["class Conv2dReLU(nn.Sequential):\n","\n","    def __init__(\n","            self,\n","            in_channels,\n","            out_channels,\n","            kernel_size,\n","            padding=0,\n","            stride=1,\n","            use_batchnorm=True,\n","    ):\n","\n","        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n","            raise RuntimeError(\n","                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n","                + \"To install see: https://github.com/mapillary/inplace_abn\"\n","            )\n","\n","        conv = nn.Conv2d(\n","            in_channels,\n","            out_channels,\n","            kernel_size,\n","            stride=stride,\n","            padding=padding,\n","            bias=not (use_batchnorm),\n","        )\n","        relu = nn.ReLU(inplace=True)\n","\n","        if use_batchnorm == \"inplace\":\n","            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n","            relu = nn.Identity()\n","        elif use_batchnorm and use_batchnorm != \"inplace\":\n","            bn = nn.BatchNorm2d(out_channels)\n","        else:\n","            bn = nn.Identity()\n","\n","        super(Conv2dReLU, self).__init__(conv, bn, relu)\n","\n","class SEModule(nn.Module):\n","\n","    def __init__(self, in_channels, reduction=16):\n","        super().__init__()\n","        self.cSE = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(in_channels, in_channels // reduction, 1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(in_channels // reduction, in_channels, 1),\n","            nn.Sigmoid(),\n","        )\n","\n","    def forward(self, x):\n","        return x * self.cSE(x)\n","\n","class sSEModule(nn.Module):\n","\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n","\n","    def forward(self, x):\n","        return x * self.sSE(x)\n","\n","class SCSEModule(nn.Module):\n"," \n","    def __init__(self, in_channels, reduction=16):\n","        super().__init__()\n","        self.cSE = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(in_channels, in_channels // reduction, 1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(in_channels // reduction, in_channels, 1),\n","            nn.Sigmoid(),\n","        )\n","        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n","\n","    def forward(self, x):\n","        return x * self.cSE(x) + x * self.sSE(x)\n","\n","class ArgMax(nn.Module):\n","\n","    def __init__(self, dim=None):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, x):\n","        return torch.argmax(x, dim=self.dim)\n","\n","\n","class Activation(nn.Module):\n","\n","    def __init__(self, name, **params):\n","\n","        super().__init__()\n","\n","        if name is None or name == 'identity':\n","            self.activation = nn.Identity(**params)\n","        elif name == 'sigmoid':\n","            self.activation = nn.Sigmoid()\n","        elif name == 'softmax2d':\n","            self.activation = nn.Softmax(dim=1, **params)\n","        elif name == 'softmax':\n","            self.activation = nn.Softmax(**params)\n","        elif name == 'logsoftmax':\n","            self.activation = nn.LogSoftmax(**params)\n","        elif name == 'tanh':\n","            self.activation = nn.Tanh()\n","        elif name == 'argmax':\n","            self.activation = ArgMax(**params)\n","        elif name == 'argmax2d':\n","            self.activation = ArgMax(dim=1, **params)\n","        elif callable(name):\n","            self.activation = name(**params)\n","        else:\n","            raise ValueError('Activation should be callable/sigmoid/softmax/logsoftmax/tanh/None; got {}'.format(name))\n","\n","    def forward(self, x):\n","        return self.activation(x)\n","\n","\n","class Attention(nn.Module):\n","\n","    def __init__(self, name, **params):\n","        super().__init__()\n","\n","        if name is None:\n","            self.attention = nn.Identity(**params)\n","        elif name == 'scse':\n","            self.attention = SCSEModule(**params)\n","        elif name == 'se':\n","            self.attention = SEModule(**params)\n","        else:\n","            raise ValueError(\"Attention {} is not implemented\".format(name))\n","\n","    def forward(self, x):\n","        return self.attention(x)\n","\n","class Flatten(nn.Module):\n","    def forward(self, x):\n","        return x.view(x.shape[0], -1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T06:20:26.604738Z","iopub.status.busy":"2024-03-26T06:20:26.604467Z","iopub.status.idle":"2024-03-26T06:20:26.621990Z","shell.execute_reply":"2024-03-26T06:20:26.621161Z","shell.execute_reply.started":"2024-03-26T06:20:26.604715Z"},"trusted":true},"outputs":[],"source":["class DecoderBlock(nn.Module):\n","    def __init__(\n","            self,\n","            in_channels,\n","            skip_channels,\n","            out_channels,\n","            use_batchnorm=True,\n","            attention_type=None,\n","    ):\n","        super().__init__()\n","\n","        self.conv1 = Conv2dReLU(\n","            in_channels + skip_channels,\n","            out_channels,\n","            kernel_size=3,\n","            padding=1,\n","            use_batchnorm=use_batchnorm,\n","        )\n","        self.attention1 = Attention(attention_type, in_channels=in_channels + skip_channels)\n","        self.conv2 = Conv2dReLU(\n","            out_channels,\n","            out_channels,\n","            kernel_size=3,\n","            padding=1,\n","            use_batchnorm=use_batchnorm,\n","        )\n","        self.attention2 = Attention(attention_type, in_channels=out_channels)\n","\n","    def forward(self, x, skip=None):\n","        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n","        if skip is not None:\n","            x = torch.cat([x, skip], dim=1)\n","            x = self.attention1(x)\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.attention2(x)\n","        return x\n","\n","\n","class CenterBlock(nn.Sequential):\n","    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n","        conv1 = Conv2dReLU(\n","            in_channels,\n","            out_channels,\n","            kernel_size=3,\n","            padding=1,\n","            use_batchnorm=use_batchnorm,\n","        )\n","        conv2 = Conv2dReLU(\n","            out_channels,\n","            out_channels,\n","            kernel_size=3,\n","            padding=1,\n","            use_batchnorm=use_batchnorm,\n","        )\n","        super().__init__(conv1, conv2)\n","\n","\n","class UnetDecoder(nn.Module):\n","    def __init__(\n","            self,\n","            encoder_channels,\n","            decoder_channels,\n","            n_blocks=5,\n","            use_batchnorm=True,\n","            attention_type=None,\n","            center=False,\n","    ):\n","        super().__init__()\n","\n","        if n_blocks != len(decoder_channels):\n","            raise ValueError(\n","                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n","                    n_blocks, len(decoder_channels)\n","                )\n","            )\n","\n","        encoder_channels = encoder_channels[1:]  # remove first skip with same spatial resolution\n","        encoder_channels = encoder_channels[::-1]  # reverse channels to start from head of encoder\n","\n","        # computing blocks input and output channels\n","        head_channels = encoder_channels[0]\n","        in_channels = [head_channels] + list(decoder_channels[:-1])\n","        skip_channels = list(encoder_channels[1:]) + [0]\n","        out_channels = decoder_channels\n","\n","        if center:\n","            self.center = CenterBlock(\n","                head_channels, head_channels, use_batchnorm=use_batchnorm\n","            )\n","        else:\n","            self.center = nn.Identity()\n","\n","        # combine decoder keyword arguments\n","        kwargs = dict(use_batchnorm=use_batchnorm, attention_type=attention_type)\n","        blocks = [\n","            DecoderBlock(in_ch, skip_ch, out_ch, **kwargs)\n","            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n","        ]\n","        self.blocks = nn.ModuleList(blocks)\n","\n","    def forward(self, *features):\n","\n","        features = features[1:]    # remove first skip with same spatial resolution\n","        features = features[::-1]  # reverse channels to start from head of encoder\n","\n","        head = features[0]\n","        skips = features[1:]\n","\n","        x = self.center(head)\n","        for i, decoder_block in enumerate(self.blocks):\n","            skip = skips[i] if i < len(skips) else None\n","            x = decoder_block(x, skip)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T06:20:26.623413Z","iopub.status.busy":"2024-03-26T06:20:26.623060Z","iopub.status.idle":"2024-03-26T06:20:26.636763Z","shell.execute_reply":"2024-03-26T06:20:26.636028Z","shell.execute_reply.started":"2024-03-26T06:20:26.623383Z"},"trusted":true},"outputs":[],"source":["def patch_first_conv(model, in_channels):\n","\n","    # get first conv\n","    for module in model.modules():\n","        if isinstance(module, nn.Conv2d):\n","            break\n","\n","    # change input channels for first conv\n","    module.in_channels = in_channels\n","    weight = module.weight.detach()\n","    reset = False\n","\n","    if in_channels == 1:\n","        weight = weight.sum(1, keepdim=True)\n","    elif in_channels == 2:\n","        weight = weight[:, :2] * (3.0 / 2.0)\n","    else:\n","        reset = True\n","        weight = torch.Tensor(\n","            module.out_channels,\n","            module.in_channels // module.groups,\n","            *module.kernel_size\n","        )\n","\n","    module.weight = nn.parameter.Parameter(weight)\n","    if reset:\n","        module.reset_parameters()\n","\n","\n","def replace_strides_with_dilation(module, dilation_rate):\n","    \"\"\"Patch Conv2d modules replacing strides with dilation\"\"\"\n","    for mod in module.modules():\n","        if isinstance(mod, nn.Conv2d):\n","            mod.stride = (1, 1)\n","            mod.dilation = (dilation_rate, dilation_rate)\n","            kh, kw = mod.kernel_size\n","            mod.padding = ((kh // 2) * dilation_rate, (kh // 2) * dilation_rate)\n","\n","            # Kostyl for EfficientNet\n","            if hasattr(mod, \"static_padding\"):\n","                mod.static_padding = nn.Identity()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T06:20:26.637976Z","iopub.status.busy":"2024-03-26T06:20:26.637707Z","iopub.status.idle":"2024-03-26T06:20:26.651694Z","shell.execute_reply":"2024-03-26T06:20:26.650908Z","shell.execute_reply.started":"2024-03-26T06:20:26.637954Z"},"trusted":true},"outputs":[],"source":["class EncoderMixin:\n","    \n","    @property\n","    def out_channels(self):\n","        \"\"\"Return channels dimensions for each tensor of forward output of encoder\"\"\"\n","        return self._out_channels[: self._depth + 1]\n","\n","    def set_in_channels(self, in_channels):\n","        \"\"\"Change first convolution channels\"\"\"\n","        if in_channels == 3:\n","            return\n","\n","        self._in_channels = in_channels\n","        if self._out_channels[0] == 3:\n","            self._out_channels = tuple([in_channels] + list(self._out_channels)[1:])\n","\n","        patch_first_conv(model=self, in_channels=in_channels)\n","\n","    def get_stages(self):\n","        \"\"\"Method should be overridden in encoder\"\"\"\n","        raise NotImplementedError\n","\n","    def make_dilated(self, stage_list, dilation_list):\n","        stages = self.get_stages()\n","        for stage_indx, dilation_rate in zip(stage_list, dilation_list):\n","            replace_strides_with_dilation(\n","                module=stages[stage_indx],\n","                dilation_rate=dilation_rate,\n","            )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T06:20:26.653664Z","iopub.status.busy":"2024-03-26T06:20:26.653084Z","iopub.status.idle":"2024-03-26T06:20:26.676122Z","shell.execute_reply":"2024-03-26T06:20:26.675330Z","shell.execute_reply.started":"2024-03-26T06:20:26.653631Z"},"trusted":true},"outputs":[],"source":["class EfficientNetEncoder(EfficientNet, EncoderMixin):\n","    def __init__(self, stage_idxs, out_channels, model_name, depth=5):\n","\n","        blocks_args, global_params = get_model_params(model_name, override_params=None)\n","        super().__init__(blocks_args, global_params)\n","\n","        self._stage_idxs = stage_idxs\n","        self._out_channels = out_channels\n","        self._depth = depth\n","        self._in_channels = 3\n","\n","        del self._fc\n","\n","    def get_stages(self):\n","        return [\n","            nn.Identity(),\n","            nn.Sequential(self._conv_stem, self._bn0, self._swish),\n","            self._blocks[:self._stage_idxs[0]],\n","            self._blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n","            self._blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n","            self._blocks[self._stage_idxs[2]:],\n","        ]\n","\n","    def forward(self, x):\n","        stages = self.get_stages()\n","\n","        block_number = 0.\n","        drop_connect_rate = self._global_params.drop_connect_rate\n","\n","        features = []\n","        for i in range(self._depth + 1):\n","\n","            # Identity and Sequential stages\n","            if i < 2:\n","                x = stages[i](x)\n","\n","            # Block stages need drop_connect rate\n","            else:\n","                for module in stages[i]:\n","                    drop_connect = drop_connect_rate * block_number / len(self._blocks)\n","                    block_number += 1.\n","                    x = module(x, drop_connect)\n","\n","            features.append(x)\n","\n","        return features\n","\n","    def load_state_dict(self, state_dict, **kwargs):\n","        state_dict.pop(\"_fc.bias\")\n","        state_dict.pop(\"_fc.weight\")\n","        super().load_state_dict(state_dict, **kwargs)\n","\n","\n","def  _get_pretrained_settings(encoder):\n","    pretrained_settings = {\n","        \"imagenet\": {\n","            \"mean\": [0.485, 0.456, 0.406],\n","            \"std\": [0.229, 0.224, 0.225],\n","            \"url\": url_map[encoder],\n","            \"input_space\": \"RGB\",\n","            \"input_range\": [0, 1],\n","        },\n","        \"advprop\": {\n","            \"mean\": [0.5, 0.5, 0.5],\n","            \"std\": [0.5, 0.5, 0.5],\n","            \"url\": url_map_advprop[encoder],\n","            \"input_space\": \"RGB\",\n","            \"input_range\": [0, 1],\n","        }\n","    }\n","    return pretrained_settings\n","\n","\n","efficient_net_encoders = {\n","    \"efficientnet-b0\": {\n","        \"encoder\": EfficientNetEncoder,\n","        \"pretrained_settings\": _get_pretrained_settings(\"efficientnet-b0\"),\n","        \"params\": {\n","            \"out_channels\": (3, 32, 24, 40, 112, 320),\n","            \"stage_idxs\": (3, 5, 9, 16),\n","            \"model_name\": \"efficientnet-b0\",\n","        },\n","    },\n","    \"efficientnet-b1\": {\n","        \"encoder\": EfficientNetEncoder,\n","        \"pretrained_settings\": _get_pretrained_settings(\"efficientnet-b1\"),\n","        \"params\": {\n","            \"out_channels\": (3, 32, 24, 40, 112, 320),\n","            \"stage_idxs\": (5, 8, 16, 23),\n","            \"model_name\": \"efficientnet-b1\",\n","        },\n","    },\n","    \"efficientnet-b2\": {\n","        \"encoder\": EfficientNetEncoder,\n","        \"pretrained_settings\": _get_pretrained_settings(\"efficientnet-b2\"),\n","        \"params\": {\n","            \"out_channels\": (3, 32, 24, 48, 120, 352),\n","            \"stage_idxs\": (5, 8, 16, 23),\n","            \"model_name\": \"efficientnet-b2\",\n","        },\n","    },\n","    \"efficientnet-b3\": {\n","        \"encoder\": EfficientNetEncoder,\n","        \"pretrained_settings\": _get_pretrained_settings(\"efficientnet-b3\"),\n","        \"params\": {\n","            \"out_channels\": (3, 40, 32, 48, 136, 384),\n","            \"stage_idxs\": (5, 8, 18, 26),\n","            \"model_name\": \"efficientnet-b3\",\n","        },\n","    },\n","    \"efficientnet-b4\": {\n","        \"encoder\": EfficientNetEncoder,\n","        \"pretrained_settings\": _get_pretrained_settings(\"efficientnet-b4\"),\n","        \"params\": {\n","            \"out_channels\": (3, 48, 32, 56, 160, 448),\n","            \"stage_idxs\": (6, 10, 22, 32),\n","            \"model_name\": \"efficientnet-b4\",\n","        },\n","    },\n","    \"efficientnet-b5\": {\n","        \"encoder\": EfficientNetEncoder,\n","        \"pretrained_settings\": _get_pretrained_settings(\"efficientnet-b5\"),\n","        \"params\": {\n","            \"out_channels\": (3, 48, 40, 64, 176, 512),\n","            \"stage_idxs\": (8, 13, 27, 39),\n","            \"model_name\": \"efficientnet-b5\",\n","        },\n","    },\n","    \"efficientnet-b6\": {\n","        \"encoder\": EfficientNetEncoder,\n","        \"pretrained_settings\": _get_pretrained_settings(\"efficientnet-b6\"),\n","        \"params\": {\n","            \"out_channels\": (3, 56, 40, 72, 200, 576),\n","            \"stage_idxs\": (9, 15, 31, 45),\n","            \"model_name\": \"efficientnet-b6\",\n","        },\n","    },\n","    \"efficientnet-b7\": {\n","        \"encoder\": EfficientNetEncoder,\n","        \"pretrained_settings\": _get_pretrained_settings(\"efficientnet-b7\"),\n","        \"params\": {\n","            \"out_channels\": (3, 64, 48, 80, 224, 640),\n","            \"stage_idxs\": (11, 18, 38, 55),\n","            \"model_name\": \"efficientnet-b7\",\n","        },\n","    },\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T06:20:26.677340Z","iopub.status.busy":"2024-03-26T06:20:26.677069Z","iopub.status.idle":"2024-03-26T06:20:26.690063Z","shell.execute_reply":"2024-03-26T06:20:26.689269Z","shell.execute_reply.started":"2024-03-26T06:20:26.677295Z"},"trusted":true},"outputs":[],"source":["def preprocess_input(x, mean=None, std=None, input_space=\"RGB\", input_range=None, **kwargs):\n","\n","    if input_space == \"BGR\":\n","        x = x[..., ::-1].copy()\n","\n","    if input_range is not None:\n","        if x.max() > 1 and input_range[1] == 1:\n","            x = x / 255.0\n","\n","    if mean is not None:\n","        mean = np.array(mean)\n","        x = x - mean\n","\n","    if std is not None:\n","        std = np.array(std)\n","        x = x / std\n","\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T06:20:26.691995Z","iopub.status.busy":"2024-03-26T06:20:26.691189Z","iopub.status.idle":"2024-03-26T06:20:26.704790Z","shell.execute_reply":"2024-03-26T06:20:26.703904Z","shell.execute_reply.started":"2024-03-26T06:20:26.691969Z"},"trusted":true},"outputs":[],"source":["def get_encoder(name, in_channels=3, depth=5, weights=None):\n","\n","    try:\n","        Encoder = encoders[name][\"encoder\"]\n","    except KeyError:\n","        raise KeyError(\"Wrong encoder name `{}`, supported encoders: {}\".format(name, list(encoders.keys())))\n","\n","    params = encoders[name][\"params\"]\n","    params.update(depth=depth)\n","    encoder = Encoder(**params)\n","\n","    if weights is not None:\n","        try:\n","            settings = encoders[name][\"pretrained_settings\"][weights]\n","        except KeyError:\n","            raise KeyError(\"Wrong pretrained weights `{}` for encoder `{}`. Available options are: {}\".format(\n","                weights, name, list(encoders[name][\"pretrained_settings\"].keys()),\n","            ))\n","        encoder.load_state_dict(model_zoo.load_url(settings[\"url\"]))\n","\n","    encoder.set_in_channels(in_channels)\n","\n","    return encoder\n","\n","\n","def get_encoder_names():\n","    return list(encoders.keys())\n","\n","def get_preprocessing_params(encoder_name, pretrained=\"imagenet\"):\n","    settings = encoders[encoder_name][\"pretrained_settings\"]\n","\n","    if pretrained not in settings.keys():\n","        raise ValueError(\"Available pretrained options {}\".format(settings.keys()))\n","\n","    formatted_settings = {}\n","    formatted_settings[\"input_space\"] = settings[pretrained].get(\"input_space\")\n","    formatted_settings[\"input_range\"] = settings[pretrained].get(\"input_range\")\n","    formatted_settings[\"mean\"] = settings[pretrained].get(\"mean\")\n","    formatted_settings[\"std\"] = settings[pretrained].get(\"std\")\n","    return formatted_settings\n","\n","\n","def get_preprocessing_fn(encoder_name, pretrained=\"imagenet\"):\n","    params = get_preprocessing_params(encoder_name, pretrained=pretrained)\n","    return functools.partial(preprocess_input, **params)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T06:20:26.720641Z","iopub.status.busy":"2024-03-26T06:20:26.720377Z","iopub.status.idle":"2024-03-26T06:20:26.733717Z","shell.execute_reply":"2024-03-26T06:20:26.732977Z","shell.execute_reply.started":"2024-03-26T06:20:26.720618Z"},"trusted":true},"outputs":[],"source":["class SegmentationModel(torch.nn.Module):\n","    \n","    def forward(self, x):\n","        \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n","        features = self.encoder(x)\n","        decoder_output = self.decoder(*features)\n","\n","        masks = self.segmentation_head(decoder_output)\n","\n","        if self.classification_head is not None:\n","            labels = self.classification_head(features[-1])\n","            return masks, labels\n","\n","        return masks\n","\n","    def predict(self, x):\n","   \n","        if self.training:\n","            self.eval()\n","\n","        with torch.no_grad():\n","            x = self.forward(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T06:20:26.735350Z","iopub.status.busy":"2024-03-26T06:20:26.734754Z","iopub.status.idle":"2024-03-26T06:20:26.744566Z","shell.execute_reply":"2024-03-26T06:20:26.743808Z","shell.execute_reply.started":"2024-03-26T06:20:26.735301Z"},"trusted":true},"outputs":[],"source":["class SegmentationHead(nn.Sequential):\n","\n","    def __init__(self, in_channels, out_channels, kernel_size=3, activation=None, upsampling=1):\n","        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n","        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n","        activation = Activation(activation)\n","        super().__init__(conv2d, upsampling, activation)\n","\n","\n","class ClassificationHead(nn.Sequential):\n","\n","    def __init__(self, in_channels, classes, pooling=\"avg\", dropout=0.2, activation=None):\n","        if pooling not in (\"max\", \"avg\"):\n","            raise ValueError(\"Pooling should be one of ('max', 'avg'), got {}.\".format(pooling))\n","        pool = nn.AdaptiveAvgPool2d(1) if pooling == 'avg' else nn.AdaptiveMaxPool2d(1)\n","        flatten = Flatten()\n","        dropout = nn.Dropout(p=dropout, inplace=True) if dropout else nn.Identity()\n","        linear = nn.Linear(in_channels, classes, bias=True)\n","        activation = Activation(activation)\n","        super().__init__(pool, flatten, dropout, linear, activation)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Unet(SegmentationModel):\n","\n","    def __init__(\n","        self,\n","        encoder_name: str = \"resnet34\",\n","        encoder_depth: int = 5,\n","        encoder_weights: Optional[str] = \"imagenet\",\n","        decoder_use_batchnorm: bool = True,\n","        decoder_channels: List[int] = (256, 128, 64, 32, 16),\n","        decoder_attention_type: Optional[str] = None,\n","        in_channels: int = 3,\n","        classes: int = 1,\n","        activation: Optional[Union[str, callable]] = None,\n","        aux_params: Optional[dict] = None,\n","    ):\n","        super().__init__()\n","\n","        self.encoder = get_encoder(\n","            encoder_name,\n","            in_channels=in_channels,\n","            depth=encoder_depth,\n","            weights=encoder_weights,\n","        )\n","\n","        self.decoder = UnetDecoder(\n","            encoder_channels=self.encoder.out_channels,\n","            decoder_channels=decoder_channels,\n","            n_blocks=encoder_depth,\n","            use_batchnorm=decoder_use_batchnorm,\n","            center=True if encoder_name.startswith(\"vgg\") else False,\n","            attention_type=decoder_attention_type,\n","        )\n","\n","        self.segmentation_head = SegmentationHead(\n","            in_channels=decoder_channels[-1],\n","            out_channels=classes,\n","            activation=activation,\n","            kernel_size=3,\n","        )\n","\n","        if aux_params is not None:\n","            self.classification_head = ClassificationHead(\n","                in_channels=self.encoder.out_channels[-1], **aux_params\n","            )\n","        else:\n","            self.classification_head = None\n","\n","        self.name = \"u-{}\".format(encoder_name)\n","        self.initialize()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T06:20:26.863504Z","iopub.status.busy":"2024-03-26T06:20:26.863206Z","iopub.status.idle":"2024-03-26T06:20:26.875837Z","shell.execute_reply":"2024-03-26T06:20:26.875087Z","shell.execute_reply.started":"2024-03-26T06:20:26.863481Z"},"trusted":true},"outputs":[],"source":["def visualize(**images):\n","    \"\"\"PLot images in one row.\"\"\"\n","    n = len(images)\n","    plt.figure(figsize=(10, 5))\n","    for i, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n, i + 1)\n","        plt.xticks([])\n","        plt.yticks([])\n","        plt.title(' '.join(name.split('_')).title())\n","        plt.imshow(image,cmap='binary')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["encoders = {}\n","encoders.update(efficient_net_encoders)\n","DEVICE='gpu'\n","ENCODER = \"efficientnet-b5\"\n","ENCODER_WEIGHTS = 'imagenet'\n","preprocessing_fn = get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T06:21:15.475423Z","iopub.status.busy":"2024-03-26T06:21:15.475027Z","iopub.status.idle":"2024-03-26T06:21:15.481386Z","shell.execute_reply":"2024-03-26T06:21:15.480374Z","shell.execute_reply.started":"2024-03-26T06:21:15.475390Z"},"trusted":true},"outputs":[],"source":["def to_tensor(x, **kwargs):\n","    return x.transpose(2, 0, 1).astype('float32')\n","\n","def get_preprocessing(preprocessing_fn):\n","\n","    _transform = [\n","        albu.Lambda(image=preprocessing_fn),\n","        albu.Lambda(image=to_tensor),\n","    ]\n","    return albu.Compose(_transform)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_model = torch.load('C:/Users/Brian/Desktop/PesU-Net/best_model.pth',map_location='gpu')\n","test=cv2.imread('C:/Users/Brian/Desktop/test.PNG')\n","#test=cv2.resize(test,(512,512))\n","test = cv2.cvtColor(test, cv2.COLOR_BGR2RGB)\n","input_process=get_preprocessing(preprocessing_fn)\n","sample = input_process(image=test)\n","image= sample['image']\n","x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n","pr_label = best_model.predict(x_tensor)\n","pr_label = (pr_label.squeeze().cpu().numpy().round())\n","visualize(\n","    input_image=test, \n","    predicted_result=pr_label\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4602760,"sourceId":7849141,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
